import os.path
import time
import torch

from ergon.code import ErgonCode
from ergon.prompt import Prompt
from symerg.base import SymErg
from utils.cache import lru_cache


class SymErgCoder(SymErg):
    """SymErgCoder class:
        - observe  action requests from SymErgion,
        - generate code based on the request,
        - request Ergons to complete actions.

    Attributes:
        ergons (list of Ergon): attached Ergons,
        model (transformers.AutoModelForCausalLM): loaded pre-trained model,
        tokenizer (transformers.AutoTokenizer): loaded pre-trained tokenizer.

    Readonly Attributes:
        name_or_path (str): name or path of the pre-trained model,
        trait (str): trait of the SymErg instance.
    """

    def update(self, message):
        """Updates the SymErgCoder based on the received message.

        Args:
            message (dict): a dictionary containing the message
                            with keys "topic", "payload", and "comment".

        Raises:
            ValueError: if the message has an unexpected payload type.
        """
        topic_branch = message.get("topic")
        payload = message.get("payload")

        if isinstance(payload, Prompt) and len(payload) > 0:
            response = {"comment": message.get("comment")}

            print(f"Response for {topic_branch} to be generated by {self.name_or_path}")
            start_time = time.time()
            generated_ids = self.generate(str(payload))
            end_time = time.time()
            delta = round(end_time - start_time, 1)

            # prompt wout code suffix
            prompted_ids = self.tokenizer(str(payload), return_tensors="pt").get("input_ids")
            suffix_ids = self.tokenizer(payload.suffix, return_tensors="pt").get("input_ids")
            if torch.equal(prompted_ids[0, -suffix_ids.shape[-1]:], suffix_ids[0]):
                prompted_ids = prompted_ids[:, :-suffix_ids.shape[-1]]

            # generated wout prompt
            if torch.equal(generated_ids[0, :prompted_ids.shape[-1]], prompted_ids[0]):
                generated_ids = generated_ids[:, prompted_ids.shape[-1]:]

            print(f"It took {delta}s to generate {generated_ids.shape[-1]} new tokens")
            generated = self.tokenizer.decode(
                generated_ids[0],
                skip_special_tokens=True
            )
            response.update({"payload": generated})

            ergon = None
            if topic_branch in [ergon.name for ergon in self.ergons]:
                [ergon] = [ergon for ergon in self.ergons if ergon.name == topic_branch]
                topic_branch = f"{os.path.basename(self.name_or_path)}_{topic_branch}"

            response.update({"topic": topic_branch})
            self._notify(response, ergon)

        elif isinstance(payload, ErgonCode):
            # assign symerg to new ergon
            if payload not in self.ergons:
                self.attach_ergon(payload)

            # assign symerg branch to ergon
            elif topic_branch not in [branch for ergon in self.ergons for branch in ergon.branches]:
                self._notify(message, payload)

            # unassign symerg from ergon after ergon branch deletion
            elif topic_branch == payload.name:
                self.detach_ergon(payload)

            # unassign symerg branch and symerg from ergon after symerg branch deletion
            else:
                self._notify(message, payload)
                self.detach_ergon(payload)

        else:
            raise ValueError(f"Message:\n{message}\nhas unexpected payload")

    def _notify(self, message, ergon_to_notify=None):
        """Notifies one or more Ergons with the given message.
        If no Ergons specified, all Ergons are notified.

        Args:
            message (dict): the message to be sent to the Ergons.
            ergon_to_notify (Ergon, optional): the Ergon to notify.
        """
        ergons = [ergon_to_notify] if ergon_to_notify else self.ergons

        for ergon in ergons:
            ergon.update(message)

    @lru_cache("_response_cache_size")
    def generate(self, prompt):
        """Generates code based on the given prompt.

        Args:
            prompt (str): the input prompt for code generation.

        Returns:
            str: the generated code.
        """
        encodings = self.tokenizer(prompt, return_tensors="pt")

        encodings_len = encodings.get("input_ids").shape[-1]
        max_new_tokens, _ = self.get_max_new_tokens(encodings_len)

        if max_new_tokens > 0:
            eos_token_id = self.config.eos_token_id
            self.eval()

            with torch.no_grad():
                generated_ids = self.model.generate(
                    **encodings,
                    max_new_tokens=max_new_tokens,
                    pad_token_id=eos_token_id
                )
            print(f"{self.name_or_path} returned {generated_ids.shape[-1]} tokens")
            return generated_ids

        return encodings.get("input_ids")
